<div align="center">

# ğŸŒŸ HaloAssist

### AI-Powered Navigation Assistant for the Visually Impaired

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![YOLO](https://img.shields.io/badge/YOLO-World-00FFFF.svg)](https://github.com/ultralytics/ultralytics)
[![Raspberry Pi](https://img.shields.io/badge/Raspberry_Pi-5-C51A4A.svg)](https://www.raspberrypi.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**[Features](#-features)** â€¢ 
**[Quick Start](#-quick-start)** â€¢ 
**[Demo](#-demo)** â€¢ 
**[Architecture](#-system-architecture)** â€¢ 
**[Hardware](#-hardware-setup)** â€¢ 
**[Documentation](#-documentation)**

---

</div>

## ğŸ¯ Overview

**HaloAssist** is an intelligent assistive technology system that helps visually impaired individuals navigate indoor environments through voice commands and haptic feedback. By combining state-of-the-art computer vision (YOLO-World) with intuitive haptic guidance, users can locate everyday objects hands-free.

### ğŸ’¡ How It Works

1. **Press** â†’ User presses a button to activate the system
2. **Speak** â†’ User says the object they're looking for (e.g., "water bottle")
3. **Detect** â†’ Real-time object detection using YOLO-World AI model
4. **Guide** â†’ Directional vibration motors guide user toward the object

---

## âœ¨ Features

- ğŸ—£ï¸ **Voice-Activated Search** - Hands-free object search via speech-to-text
- ğŸ¤– **Advanced AI Detection** - YOLO-World open-vocabulary object detection
- ğŸ“³ **Haptic Feedback** - Directional vibration guidance (2-8 motor array)
- ğŸ¯ **Custom Object Classes** - Optimized for home and supermarket environments
- ğŸš€ **Multi-Platform Support** - Runs on Raspberry Pi (production) and Mac (development)
- âš¡ **Real-Time Performance** - Optimized inference for embedded systems
- ğŸ”§ **Modular Architecture** - Clean separation of perception, hardware, and visualization

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Camera (USB webcam or Raspberry Pi Camera Module)
- [Optional] Raspberry Pi 5 with GPIO peripherals

### Installation

```bash
# Clone the repository
git clone https://github.com/JackSuuu/SDP-project-Haloassist.git
cd SDP-project-Haloassist

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
cd perception
pip install -r requirements.txt
```

### Run Demo

**Mac/Linux Development:**
```bash
python demo/demo_local.py
```

**Raspberry Pi (Full System):**
```bash
python demo/demo.py
```

**Image-Based Testing:**
```bash
python demo/demo_image_detector.py test_images/your_image.jpg
```

Press **`q`** to quit any demo.

---

## ğŸ¬ Demo

### Voice Command Detection
```
User: "Find my phone"
System: ğŸ” Searching for phone...
        ğŸ“³ Vibrating left â†’ Object detected at 45Â° left
        ğŸ“³ Vibrating center â†’ Object centered, within reach!
```

### Supported Objects
- **Home:** chair, couch, bed, door, stairs, phone, laptop, book
- **Kitchen:** refrigerator, microwave, bottle, cup, bowl, knife, fork
- **Food:** apple, banana, orange, broccoli, carrot

See [PRIORITY_OBJECTS](perception/config/hardware_config.py#L108) for the full list.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      HaloAssist System                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Hardware   â”‚      â”‚  Perception  â”‚      â”‚Visualization â”‚
    â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
    â”‚ â€¢ Button     â”‚â”€â”€â”€â”€â”€â–¶â”‚ â€¢ Camera     â”‚â”€â”€â”€â”€â”€â–¶â”‚ â€¢ Web UI     â”‚
    â”‚ â€¢ Speech     â”‚      â”‚ â€¢ YOLO Model â”‚      â”‚ â€¢ Debug View â”‚
    â”‚ â€¢ Haptics    â”‚â—€â”€â”€â”€â”€â”€â”‚ â€¢ Detector   â”‚      â”‚ â€¢ Metrics    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
.
â”œâ”€â”€ perception/          # Core detection system
â”‚   â”œâ”€â”€ src/            # Source code
â”‚   â”‚   â”œâ”€â”€ hardware/   # Button, speech, haptic interfaces
â”‚   â”‚   â””â”€â”€ perception/ # Camera and detection modules
â”‚   â”œâ”€â”€ config/         # Hardware & model configuration
â”‚   â””â”€â”€ test/           # Unit tests
â”‚
â”œâ”€â”€ hardware/           # Low-level GPIO drivers
â”‚   â”œâ”€â”€ button.py       # GPIO button control
â”‚   â”œâ”€â”€ stt.py         # Vosk speech-to-text
â”‚   â””â”€â”€ yolo_haptic.py # Motor control
â”‚
â”œâ”€â”€ visualization/      # Web-based monitoring
â”‚   â”œâ”€â”€ server.py       # Flask backend
â”‚   â””â”€â”€ static/         # Frontend UI
â”‚
â”œâ”€â”€ demo/              # Demo scripts
â”‚   â”œâ”€â”€ demo.py        # Full system demo (Pi)
â”‚   â”œâ”€â”€ demo_local.py  # Mac development demo
â”‚   â””â”€â”€ demo_image_detector.py  # Image testing
â”‚
â”œâ”€â”€ experiments/       # Performance benchmarks
â”‚   â””â”€â”€ pi5-vlm-test/ # Latency tests & results
â”‚
â””â”€â”€ docs/             # Documentation
    â”œâ”€â”€ QUICKSTART.md
    â”œâ”€â”€ HARDWARE_INTEGRATION.md
    â””â”€â”€ MODEL_CONFIG.md
```

---

## ğŸ”§ Hardware Setup

### Raspberry Pi Configuration

| Component | GPIO Pin | Description |
|-----------|----------|-------------|
| Button    | GPIO 5   | Activation button (pull-up, active LOW) |
| Left Motor| GPIO 22  | Left haptic feedback |
| Right Motor| GPIO 26 | Right haptic feedback |
| Camera    | CSI Port | Pi Camera Module 3 |

### Supported Platforms

| Platform | Model | Image Size | FPS | Motors |
|----------|-------|------------|-----|--------|
| Raspberry Pi 3 | YOLO Nano | 160Ã—160 | ~10 | 2 |
| Raspberry Pi 4 | YOLO Small | 320Ã—320 | ~15 | 2 |
| Raspberry Pi 5 | YOLO Medium | 640Ã—640 | ~30 | 2-8 |
| Mac/Linux Dev | YOLO World | 640Ã—640 | 60+ | Simulated |

See [hardware_config.py](perception/config/hardware_config.py) for platform-specific profiles.

---

## ğŸ“š Documentation

- **[Quick Start Guide](docs/QUICKSTART.md)** - Installation and first run
- **[Hardware Integration](docs/HARDWARE_INTEGRATION.md)** - GPIO setup and wiring
- **[Model Configuration](docs/MODEL_CONFIG.md)** - YOLO models and tuning
- **[Quick Reference](docs/QUICKREF.md)** - API and command reference

---

## ğŸ§ª Testing

Run unit tests:
```bash
cd perception

# Test camera interface
python test/test_camera.py

# Test object detection
python test/test_detector.py

# Test haptic feedback
python test/test_haptic.py

# Test image-based detection
python test/test_image_detector.py
```

---

## âš™ï¸ Configuration

### Easy Platform Switching

```python
# In perception/config/hardware_config.py
apply_profile('pi5')  # Options: 'pi3', 'pi4', 'pi5', 'mac'
```

### Custom Object Detection

```python
# Add your own objects to detect
PRIORITY_OBJECTS = [
    'custom_object_1',
    'custom_object_2',
    # ... your objects here
]
```

### Adjust Haptic Feedback

```python
HAPTIC_CONFIG = {
    'default_strength': 0.5,    # Motor intensity (0.0 - 1.0)
    'default_duration': 0.25,   # Vibration duration (seconds)
    'detection_interval': 0.25, # Update frequency
}
```

---

## ğŸ“ Research & Benchmarks

Performance benchmarks and latency tests available in [`experiments/pi5-vlm-test/`](experiments/pi5-vlm-test/):
- YOLO detection latency measurements
- Model comparison (Nano vs Small vs Medium)
- Raspberry Pi 5 performance analysis

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- [ ] Expand object detection categories
- [ ] Multi-language speech support
- [ ] 8-motor haptic array implementation
- [ ] Mobile app companion
- [ ] Battery optimization for portable use

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team

**Senior Design Project (SDP)** - Spring 2026

---

## ğŸ™ Acknowledgments

- [Ultralytics YOLO](https://github.com/ultralytics/ultralytics) - Object detection framework
- [Vosk](https://alphacephei.com/vosk/) - Offline speech recognition
- [Raspberry Pi Foundation](https://www.raspberrypi.org/) - Hardware platform

---

<div align="center">

**Built with â¤ï¸ for accessibility**

[â¬† Back to Top](#-haloassist)

</div>
