# Perception Framework for Blind Assistance

An object detection framework designed to assist blind individuals in navigating home and supermarket environments using **YOLO-World** open-vocabulary detection and haptic feedback.

## Features
- **Speech-to-Text object selection** - User specifies target object via voice
- Real-time object detection using **YOLO-World** (open-vocabulary)
- **Directional haptic guidance** - Vibration indicates where object is located
- Custom object classes for home/supermarket scenarios

---

- Vibration motor array feedback (2 motors for Pi3, 8 motors for Pi5)
- Optimized for Raspberry Pi 5 / Linux
- Mac camera support for development/testing

## Workflow

1. ğŸ”˜ **Button Press** - User presses button to start
2. ğŸ¤ **Speech Input** - User says object name ("bottle", "cup", "phone", etc.)
3. ğŸ“¹ **Detection** - Camera searches for that specific object using YOLO
4. ğŸ“³ **Haptic Guidance** - Motors vibrate to indicate direction:
   - **Left**: Object on left side
   - **Right**: Object on right side
   - **Both**: Object centered
5. ğŸ”„ **Continuous** - System keeps detecting and guiding until object found

## Project Structure
```
perception/
â”œâ”€â”€ src/           # Source code
â”œâ”€â”€ test/          # Test files
â”œâ”€â”€ config/        # Configuration files
â”œâ”€â”€ models/        # YOLO model weights
â””â”€â”€ requirements.txt
```

## Hardware Requirements
- Raspberry Pi 5 (or Mac for development/testing)
- Camera module
- 6-8 vibration motors
- Motor driver board

## Setup
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Mac/Linux
# venv\Scripts\activate  # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## Quick Start

**Mac (Testing):**
```bash
python demo.py
```

**Raspberry Pi (Production):**
```bash
python src/main.py --no-display
```

See [QUICKSTART.md](QUICKSTART.md) for detailed instructions.

## System Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Button    â”‚ â”€â”€> User presses button
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Speech-to- â”‚ â”€â”€> User says "bottle"
                    â”‚    Text     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Camera    â”‚â”€â”€> â”‚  YOLO-World Model   â”‚ â”€â”€> Detects "bottle" only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ (Target: bottle)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â†“
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Object Located? â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚            â”‚
                    Yes          No
                     â”‚            â”‚
                     â†“            â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   Keep searching
              â”‚ Position â”‚
              â”‚ Analysis â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Haptic Controller    â”‚
         â”‚ - Left motor (L)     â”‚
         â”‚ - Right motor (R)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
         [Directional Guidance]
         L: Left object
         R: Right object
         L+R: Center object
```

## Core Modules

- **detector.py**: YOLO-World based object detection with custom classes
- **haptic.py**: Vibration motor control for directional guidance  
- **camera.py**: Cross-platform camera interface (Mac/RPi)
- **main.py**: Main integration and control loop

## Why YOLO-World?

- **Open-vocabulary detection**: Detect custom object categories without retraining
- **Flexible**: Easily add new object classes for different scenarios
- **Accurate**: Better performance for specific home/supermarket items
- **Efficient**: Real-time performance on Raspberry Pi 5
